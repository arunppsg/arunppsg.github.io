<!DOCTYPE html> 
<html>
<head>
    <meta charset="utf-8">
    <link rel="stylesheet" href="https://www.arunppsg.in/css/base.css">
		<title>Arun Palaniappan</title>
</head>

<body>
    <header>
        <nav>
            <div class="nav-links">
                
                
                <a class="menu-links" 
                    href="https://www.arunppsg.in/">Home
                </a>
                
                
                <a class="menu-links" 
                    href="https://www.arunppsg.in/posts/">Posts
                </a>
                
                
                <a class="menu-links" 
                    href="https://www.arunppsg.in/projects/">Projects
                </a>
                
                
                <a class="menu-links" 
                    href="https://www.arunppsg.in/now/">Now
                </a>
                
                
                <a class="menu-links" 
                    href="https://www.arunppsg.in/musings/">Musings
                </a>
                
                
                <a class="menu-links" 
                    href="https://www.arunppsg.in/book-notes/">Book Notes
                </a>
                
            </div>
        </nav>
    </header>
    <article>
<h1 class="title">
  Log: My Dumb Mistakes in Machine Learning
</h1>
<p class="subtitle"><strong>2024-03-04</strong></p>
<p>A log of my dumb mistakes in machine learning.</p>
<ol>
<li></li>
</ol>
<p>I had the below piece of code in a training loop:</p>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span>outputs = </span><span style="color:#bf616a;">model</span><span>(**inputs)
</span><span>loss = outputs.</span><span style="color:#bf616a;">get</span><span>(&quot;</span><span style="color:#a3be8c;">loss</span><span>&quot;)
</span><span>loss.</span><span style="color:#bf616a;">backward</span><span>()
</span><span>optimizer.</span><span style="color:#bf616a;">zero_grad</span><span>()
</span><span>optimizer.</span><span style="color:#bf616a;">step</span><span>()
</span></code></pre>
<p>The model was not getting trained despite debugging by varying the learning rates, changing optimizers and checking other issues. But the issue was <code>optimizer.zero_grad()</code> cleared out the gradient accumulated by <code>loss.backward()</code>.
Correct code:</p>
<pre data-lang="py" style="background-color:#2b303b;color:#c0c5ce;" class="language-py "><code class="language-py" data-lang="py"><span>optimizer.</span><span style="color:#bf616a;">zero_grad</span><span>()
</span><span>outputs = </span><span style="color:#bf616a;">model</span><span>(**inputs)
</span><span>loss = outputs.</span><span style="color:#bf616a;">get</span><span>(&quot;</span><span style="color:#a3be8c;">loss</span><span>&quot;)
</span><span>loss.</span><span style="color:#bf616a;">backward</span><span>()
</span><span>optimizer.</span><span style="color:#bf616a;">step</span><span>()
</span></code></pre>
<ol start="2">
<li>Not normalizing y-values in validation dataset.</li>
</ol>
<p>I was training a regression model using deep neural networks.
The training loss decreased, training evaluation metrics were good and the training progressed well. But the RMSE in validation metrics was very high (100x the RMSE of training evaluation). It turned out that I didn't normalize the values in validation dataset.</p>
<ol start="3">
<li>Training loss not improving in a contrastive distillation based setting</li>
</ol>
<p>There was the teacher model, the student model.
The issue here was I had only passed the parameters of the teacher model to the optimizer and not the student model, and thus the students model was not learning.</p>
<ol start="4">
<li>Subnet configuration in multi-node training</li>
</ol>
<p>Multi-node training - I was debugging multi-node training and for some reason the training did not start. I could see no checkpoints nor anything in training logs except that data is getting loaded. The issue was in configuring of AWS Subnet - there was a misconfiguration in the subnet that made the nodes not to communicate with each other.</p>
<p>A list of mistakes which I have posted in forums:</p>
<ul>
<li><a href="https://github.com/Open-Catalyst-Project/ocp/issues/332">https://github.com/ray-project/ray/issues/43753</a></li>
<li><a href="https://github.com/Open-Catalyst-Project/ocp/issues/332">https://github.com/Open-Catalyst-Project/ocp/issues/332</a></li>
<li><a href="https://discuss.pytorch.org/t/using-autograd-function-to-compute-gradient-for-a-function/178979">https://discuss.pytorch.org/t/using-autograd-function-to-compute-gradient-for-a-function/178979</a></li>
</ul>

</article> 

    <footer>
        <!-- Copyright &copy; 2023 Arun Thiagarajan -->
    </footer>
</body>
</html>
